import os
import re
import yaml
import sys
import json
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

# .envã®èª­ã¿è¾¼ã¿
load_dotenv()

# Windowså¯¾å¿œ
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

def get_text_analysis_from_ai(client, text_chunk):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰èª­ã¿é–“é•ã„ãã†ãªå˜èªã‚’AIã«æŠ½å‡ºã•ã›ã‚‹"""
    prompt = f"""
ä»¥ä¸‹ã®å°èª¬ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€TTSï¼ˆéŸ³å£°åˆæˆï¼‰ãŒèª­ã¿é–“é•ãˆãã†ãªã€Œäººåã€ã€Œåœ°åã€ã€Œç‰¹æ®Šãªç”¨èªã€ã€Œæ•°å­—ã®èª­ã¿ï¼ˆå˜ä½å«ã‚€ï¼‰ã€ã‚’ã™ã¹ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ç‰¹ã«ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é‡ç‚¹çš„ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼š
- æ—¥æœ¬äººã®å§“åï¼ˆä¾‹: æ–‰è—¤ã€ç¾å’²ã€æ‚ å¤ªï¼‰
- ç•°ä¸–ç•Œã‚‚ã®ç‰¹æœ‰ã®é€ èªï¼ˆä¾‹: é­”å°çŸ³ã€ã‚®ãƒ«ãƒ‰ã€ãƒ¬ãƒ ãƒªã‚¢ï¼‰
- æ–‡è„ˆã§èª­ã¿ãŒå¤‰ã‚ã‚‹æ¼¢å­—ï¼ˆä¾‹: ä¸€äººæš®ã‚‰ã—ã€æ˜¨æ—¥ã€æ˜æ—¥ã€ååˆ†ï¼‰
- æ•°å­—+å˜ä½ï¼ˆä¾‹: 10æ—¥ã€25æ­³ã€3ãƒ¶æœˆã€äºŒå€ï¼‰
- æ–™ç†åã‚„ææ–™ï¼ˆä¾‹: ãŠã«ãã‚Šã€æ¢…å¹²ã—ã€éš ã—å‘³ï¼‰

ãƒ†ã‚­ã‚¹ãƒˆ:
---
{text_chunk}
---
å‡ºåŠ›ã¯ã€Œå˜èª: æ­£ã—ã„ã²ã‚‰ãŒãªèª­ã¿ã€ã®JSONå½¢å¼ã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": "ã‚ãªãŸã¯ãƒ—ãƒ­ã®å°èª¬æ ¡æ­£è€…ã§ã™ã€‚"},
                      {"role": "user", "content": prompt}],
            response_format={ "type": "json_object" }
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"âš ï¸ è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰: {e}")
        return {}

def generate_corrections_v2(text_path):
    api_key = os.environ.get("OPENAI_API_KEY")
    client = OpenAI(api_key=api_key)
    
    with open(text_path, 'r', encoding='utf-8') as f:
        full_text = f.read()

    # å…¨æ–‡ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€é‡è¦ãªå“è©ã‚’Janomeã§å…ˆã«çµã‚Šè¾¼ã‚€ã®ã‚‚è‰¯ã„ãŒã€
    # ã“ã“ã§ã¯ã€Œå…¨æ–‡ã‹ã‚‰ä¸»è¦ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’AIã«æŠ½å‡ºã•ã›ã‚‹ã€æ–¹å¼ã‚’ã¨ã‚‹
    # æ–‡é‡ãŒå¤šã„å ´åˆã¯ã€å…ˆé ­ã€ä¸­é–“ã€æœ«å°¾ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    text_samples = []
    chunk_size = 2000
    text_samples.append(full_text[:chunk_size]) # é–‹å§‹
    if len(full_text) > chunk_size * 2:
        text_samples.append(full_text[len(full_text)//2 : len(full_text)//2 + chunk_size]) # ä¸­é–“
    if len(full_text) > chunk_size * 3:
        text_samples.append(full_text[-chunk_size:]) # æœ«å°¾

    all_corrections = {}
    print(f"ğŸ” å…¨æ–‡ãƒ‡ã‚£ãƒ¼ãƒ—ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
    for i, sample in enumerate(text_samples):
        print(f"   ã‚µãƒ³ãƒ—ãƒ« {i+1} ã‚’è§£æä¸­...")
        res = get_text_analysis_from_ai(client, sample)
        all_corrections.update(res)

    # æ•°å­—ã®ç‰¹æ®Šèª­ã¿ã‚’è£œå®Œï¼ˆæ­£è¦è¡¨ç¾ã§è‡ªå‹•æŠ½å‡ºï¼‰
    # ä¾‹: 10æ—¥ -> ã¨ãŠã‹
    numbers_found = re.findall(r'\d+[å¹´æœˆæ—¥æ—¥äººæ­³å€å›åˆ†ç§’]', full_text)
    if numbers_found:
        print(f"ğŸ”¢ æ•°å­—è¡¨ç¾ã‚’è£œå®Œä¸­...")
        num_prompt = f"ä»¥ä¸‹ã®è¡¨ç¾ã®æ­£ã—ã„èª­ã¿ï¼ˆã²ã‚‰ãŒãªï¼‰ã‚’æ•™ãˆã¦ãã ã•ã„: {', '.join(set(numbers_found))}"
        res = get_text_analysis_from_ai(client, num_prompt)
        all_corrections.update(res)

    # ä¿å­˜
    filename_stem = Path(text_path).stem
    title = re.sub(r'^\d{8}_', '', filename_stem)
    yaml_data = {
        "title": title,
        "category": "ç¾å®Ÿä¸–ç•Œ[æ‹æ„›]",
        "original_date": filename_stem.split('_')[0] if '_' in filename_stem else "",
        "corrections": all_corrections
    }
    
    yaml_path = Path(text_path).with_suffix('.yaml')
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(yaml_data, f, allow_unicode=True, sort_keys=False, default_flow_style=False)
    
    print(f"âœ¨ æœ€å¼·è¾æ›¸ãŒå®Œæˆã—ã¾ã—ãŸ: {yaml_path}")
    print(f"   ç™»éŒ²å˜èªæ•°: {len(all_corrections)}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        generate_corrections_v2(sys.argv[1])
    else:
        print("ä½¿ã„æ–¹: python generate_corrections.py novels/å°èª¬ãƒ•ã‚¡ã‚¤ãƒ«.txt")
