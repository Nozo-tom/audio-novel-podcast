"""
èª­ã¿é–“é•ã„ãƒ¬ãƒãƒ¼ãƒˆ + åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¾æ›¸ã‚’è‡ªå‹•è£œå¼·ã™ã‚‹ãƒ„ãƒ¼ãƒ«

ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‘
  1. èª­ã¿é–“é•ã„ãƒ¬ãƒãƒ¼ãƒˆï¼ˆåŸæ–‡ vs Whisperèªè­˜çµæœï¼‰ã‚’è§£æ
  2. åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚‚èª­ã¿è¾¼ã¿ã€å·®ç•°ç®‡æ‰€ã®å‰å¾Œã®æ–‡è„ˆã‚’å–å¾—
  3. GPT-4oã«ã€ŒåŸæ–‡ã®æ¼¢å­—èªå¥ã€ã€Œå·®ç•°ç®‡æ‰€ã®æ–‡è„ˆã€ã‚’é€ã‚Šã€
     æ­£ç¢ºãªèª­ã¿ã‚’é•·ã„æ–‡è¨€ã§è¾æ›¸ç™»éŒ²ã™ã‚‹
  4. ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: ç™»éŒ²ã‚­ãƒ¼ãŒåŸæ–‡ã«å®Ÿåœ¨ã™ã‚‹ã‹æ¤œè¨¼

ã€ä½¿ã„æ–¹ã€‘
  python fix_corrections_from_report.py novel.yaml --report report.txt --text novel.txt
  python fix_corrections_from_report.py novel.yaml --report report.txt
"""
import os
import json
import re
import yaml
import sys
import argparse
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Windowså¯¾å¿œ: UTF-8å‡ºåŠ›è¨­å®š
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªèº«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
SCRIPT_DIR = Path(__file__).parent


def parse_report(report_content):
    """ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰å·®ç•°æƒ…å ±ã‚’æ§‹é€ åŒ–ã—ã¦æŠ½å‡º"""
    differences = []
    current = {}
    
    for line in report_content.split('\n'):
        line = line.strip()
        
        # å·®ç•°ãƒ˜ãƒƒãƒ€: ã€1ã€‘ãƒãƒ£ãƒ³ã‚¯ 3 (ä¸€è‡´ç‡: 45.2%)
        match = re.match(r'ã€(\d+)ã€‘.*ä¸€è‡´ç‡:\s*([\d.]+)%', line)
        if match:
            if current:
                differences.append(current)
            current = {
                'num': int(match.group(1)),
                'ratio': float(match.group(2)),
                'original': '',
                'transcribed': ''
            }
            continue
        
        if line.startswith('åŸæ–‡:'):
            current['original'] = line[len('åŸæ–‡:'):].strip()
        elif line.startswith('èªè­˜:'):
            current['transcribed'] = line[len('èªè­˜:'):].strip()
    
    if current and current.get('original'):
        differences.append(current)
    
    return differences


def filter_meaningful_differences(differences):
    """æ„å‘³ã®ã‚ã‚‹å·®ç•°ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered = []
    
    for diff in differences:
        ratio = diff['ratio']
        orig = diff['original']
        trans = diff['transcribed']
        
        # ä¸€è‡´ç‡ãŒ98%ä»¥ä¸Šã¯ç„¡è¦–ï¼ˆã»ã¼ä¸€è‡´ï¼‰
        if ratio >= 98:
            continue
        
        # ä¸€è‡´ç‡ãŒ20%æœªæº€ã¯ãƒãƒ£ãƒ³ã‚¯ãšã‚Œã®å¯èƒ½æ€§ãŒé«˜ã„ã®ã§é™¤å¤–
        if ratio < 20:
            continue
        
        # åŸæ–‡ãŒçŸ­ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
        if len(orig) < 3:
            continue
        
        filtered.append(diff)
    
    return filtered


def find_context_in_text(original_text, sentence, context_chars=100):
    """åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆå†…ã§è©²å½“æ–‡ã®å‰å¾Œã®æ–‡è„ˆã‚’å–å¾—"""
    # å¥èª­ç‚¹ãªã©ã‚’é™¤ã„ã¦æ¤œç´¢
    search_text = sentence[:20]  # å…ˆé ­20æ–‡å­—ã§æ¤œç´¢
    pos = original_text.find(search_text)
    
    if pos == -1:
        # éƒ¨åˆ†ä¸€è‡´ã§å†è©¦è¡Œ
        for length in range(15, 5, -1):
            search_text = sentence[:length]
            pos = original_text.find(search_text)
            if pos != -1:
                break
    
    if pos == -1:
        return sentence  # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    
    start = max(0, pos - context_chars)
    end = min(len(original_text), pos + len(sentence) + context_chars)
    return original_text[start:end]


def validate_corrections(corrections, original_text, existing_corrections):
    """è¾æ›¸ã‚¨ãƒ³ãƒˆãƒªã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
    validated = {}
    rejected = []
    
    for word, reading in corrections.items():
        # å€¤ãŒæ–‡å­—åˆ—ã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not isinstance(reading, str) or not isinstance(word, str):
            rejected.append((word, reading, "å‹ãŒä¸æ­£"))
            continue
        
        # ç©ºã®ã‚­ãƒ¼ã‚„å€¤ã¯ã‚¹ã‚­ãƒƒãƒ—
        if not word.strip() or not reading.strip():
            rejected.append((word, reading, "ç©ºæ–‡å­—"))
            continue
        
        # ã‚­ãƒ¼ãŒåŸæ–‡ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if word not in original_text:
            rejected.append((word, reading, "åŸæ–‡ã«å­˜åœ¨ã—ãªã„"))
            continue
        
        # ã‚­ãƒ¼ãŒ1æ–‡å­—ã®å ´åˆã¯é™¤å¤–ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒªã‚¹ã‚¯é«˜ã™ãï¼‰
        if len(word) == 1:
            rejected.append((word, reading, "1æ–‡å­—ã¯éƒ¨åˆ†ä¸€è‡´ãƒªã‚¹ã‚¯"))
            continue
        
        # æ—¢ã«å…¨ãåŒã˜ç™»éŒ²ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if word in existing_corrections and existing_corrections[word] == reading:
            continue
        
        # ã‚­ãƒ¼ãŒã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠã®ã¿ã®å ´åˆã¯ä¸è¦
        if re.match(r'^[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼]+$', word):
            rejected.append((word, reading, "ã‹ãªæ–‡å­—ã®ã¿ï¼ˆTTSèª­ã‚ã‚‹ï¼‰"))
            continue
        
        # èª­ã¿ï¼ˆå€¤ï¼‰ãŒæ¼¢å­—ã‚’å«ã‚€å ´åˆã¯é™¤å¤–ï¼ˆã²ã‚‰ãŒãªã§ã‚ã‚‹ã¹ãï¼‰
        if re.search(r'[ä¸€-é¾¥]', reading):
            rejected.append((word, reading, "èª­ã¿ã«æ¼¢å­—ãŒå«ã¾ã‚Œã‚‹"))
            continue
        
        # ã‚­ãƒ¼ã«æ¼¢å­—ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ä¸è¦
        if not re.search(r'[ä¸€-é¾¥]', word):
            rejected.append((word, reading, "ã‚­ãƒ¼ã«æ¼¢å­—ãªã—"))
            continue
        
        # ã‚­ãƒ¼ã«å¯¾ã—ã¦æ¼¢å­—ã®å‰²åˆãŒä½ã™ãã‚‹å ´åˆï¼ˆæ–‡ã¾ã‚‹ã”ã¨ã²ã‚‰ãŒãªåŒ–ã‚’é˜²æ­¢ï¼‰
        kanji_count = len(re.findall(r'[ä¸€-é¾¥]', word))
        if len(word) > 8 and kanji_count / len(word) < 0.2:
            rejected.append((word, reading, f"æ¼¢å­—ç‡ãŒä½ã„({kanji_count}/{len(word)}) â†’ ä¸è¦ãªé•·æ–‡"))
            continue
        
        # å€¤ãŒå…¨ã¦ã²ã‚‰ãŒãªã§ã€ã‚­ãƒ¼ã®é•·ã•ãŒ15æ–‡å­—è¶… â†’ æ–‡ã¾ã‚‹ã”ã¨ã²ã‚‰ãŒãªåŒ–
        if len(word) > 15 and re.match(r'^[ã-ã‚“ãƒ¼ã€ã€‚ï¼ï¼Ÿ\s]+$', reading):
            rejected.append((word, reading, "æ–‡ã¾ã‚‹ã”ã¨ã²ã‚‰ãŒãªåŒ–ã¯ç¦æ­¢"))
            continue
        
        validated[word] = reading
    
    return validated, rejected


def sync_from_report(yaml_path, report_path=None, text_path=None):
    """èª­ã¿é–“é•ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’è§£æã—ã¦YAMLè¾æ›¸ã‚’è‡ªå‹•è£œå¼·ã™ã‚‹"""
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã®è§£æ±º
    if report_path:
        report_path = Path(report_path)
    else:
        candidate = SCRIPT_DIR / "reading_errors_report.txt"
        if candidate.exists():
            report_path = candidate
        else:
            report_path = Path("reading_errors_report.txt")
    
    if not report_path.exists():
        print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {report_path}")
        print(f"   æ¤œç´¢å ´æ‰€: {report_path.resolve()}")
        return
    
    print(f"ğŸ”„ ãƒ¬ãƒãƒ¼ãƒˆã‚’è§£æã—ã¦ {yaml_path} ã‚’è‡ªå‹•ä¿®æ­£ã—ã¾ã™...")
    print(f"   ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    
    with open(report_path, "r", encoding="utf-8") as f:
        report_content = f.read()
    
    if not report_content.strip():
        print("âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯ç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
    
    # åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿è¾¼ã¿
    original_text = ""
    if text_path:
        text_path = Path(text_path)
        if text_path.exists():
            with open(text_path, "r", encoding="utf-8") as f:
                original_text = f.read()
            print(f"   åŸæ–‡: {text_path.name} ({len(original_text):,}æ–‡å­—)")
    
    # æ—¢å­˜YAMLã®èª­ã¿è¾¼ã¿
    existing_corrections = {}
    if os.path.exists(yaml_path):
        with open(yaml_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
        existing_corrections = data.get("corrections", {})
        print(f"   æ—¢å­˜è¾æ›¸: {len(existing_corrections)}ä»¶")
    else:
        data = {}
    
    # ãƒ¬ãƒãƒ¼ãƒˆã®è§£æ
    differences = parse_report(report_content)
    print(f"   æ¤œå‡ºå·®ç•°: {len(differences)}ä»¶")
    
    # æ„å‘³ã®ã‚ã‚‹å·®ç•°ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
    meaningful = filter_meaningful_differences(differences)
    print(f"   æœ‰åŠ¹å·®ç•°: {len(meaningful)}ä»¶")
    
    if not meaningful:
        print("â„¹ï¸ ä¿®æ­£ãŒå¿…è¦ãªå·®ç•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # GPTã«é€ä¿¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
    # å·®ç•°ä¸€è¦§ï¼ˆæ–‡è„ˆä»˜ãï¼‰
    diff_entries = []
    for diff in meaningful:
        entry = {
            "åŸæ–‡": diff['original'],
            "Whisperèªè­˜çµæœ": diff['transcribed'],
            "ä¸€è‡´ç‡": f"{diff['ratio']:.1f}%"
        }
        # åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°æ–‡è„ˆã‚’è¿½åŠ 
        if original_text:
            context = find_context_in_text(original_text, diff['original'])
            entry["å‰å¾Œã®æ–‡è„ˆ"] = context
        diff_entries.append(entry)
    
    # æ—¢å­˜è¾æ›¸ã®æƒ…å ±
    existing_list = "\n".join([f"  {k}: {v}" for k, v in existing_corrections.items()])
    
    client = OpenAI()
    
    prompt = f"""\
ä»¥ä¸‹ã¯æ—¥æœ¬èªå°èª¬ã®TTSéŸ³å£°åŒ–ã§ç™ºç”Ÿã—ãŸã€Œèª­ã¿é–“é•ã„ã€ã®å·®ç•°ä¸€è¦§ã§ã™ã€‚

ã‚ãªãŸã®ä»•äº‹ã¯ã€TTSãŒèª­ã¿é–“é•ãˆãŸ**æ¼¢å­—èªå¥ã ã‘**ã‚’ç‰¹å®šã—ã€æ­£ã—ã„ã²ã‚‰ãŒãªèª­ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã§ã™ã€‚

ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
- ã‚­ãƒ¼ã¯**èª­ã¿é–“é•ãˆãŸæ¼¢å­—èªå¥ã®ã¿**ï¼ˆ2ã€œ10æ–‡å­—ç¨‹åº¦ï¼‰
- TTSãŒæ­£ã—ãèª­ã‚ã‚‹éƒ¨åˆ†ã¯å«ã‚ãªã„
- æ–‡ã¾ã‚‹ã”ã¨ã‚’ã‚­ãƒ¼ã«ã™ã‚‹ã®ã¯**çµ¶å¯¾ç¦æ­¢**

ã€å…·ä½“ä¾‹ã€‘
âœ… æ­£ã—ã„å‡ºåŠ›:
  "æ‚ªå½¹ä»¤å¬¢": "ã‚ãã‚„ãã‚Œã„ã˜ã‚‡ã†"
  "å«ŒãŒã‚‰ã›": "ã„ã‚„ãŒã‚‰ã›"
  "å¾®ç¬‘ã‚“ã ": "ã»ã»ãˆã‚“ã "
  "å¿ƒã®ä¸­ã§å‘Ÿã„ãŸ": "ã“ã“ã‚ã®ãªã‹ã§ã¤ã¶ã‚„ã„ãŸ"

âŒ é–“é•ã£ãŸå‡ºåŠ›:
  "ç§ã®åå‰ã¯ã‚¨ãƒªã‚¶ãƒ™ãƒ¼ãƒˆ": "ã‚ãŸã—ã®ãªã¾ãˆã¯ãˆã‚Šã–ã¹ãƒ¼ã¨"  â† æ–‡ã¾ã‚‹ã”ã¨ç¦æ­¢
  "ãŠå¬¢æ§˜ã€ãŠèŒ¶ã®æ™‚é–“ã§ã™": "ãŠã˜ã‚‡ã†ã•ã¾ã€ãŠã¡ã‚ƒã®ã˜ã‹ã‚“ã§ã™"  â† TTSã¯æ­£ã—ãèª­ã‚ã‚‹
  "ã‚¨ãƒªã‚¶ãƒ™ãƒ¼ãƒˆ": "ãˆã‚Šã–ã¹ãƒ¼ã¨"  â† ã‚«ã‚¿ã‚«ãƒŠã¯TTSãŒèª­ã‚ã‚‹
  "ç‹å­": "ãŠã†ã˜"  â† ä¸€èˆ¬çš„ãªæ¼¢å­—ã¯TTSãŒèª­ã‚ã‚‹

ã€ç™»éŒ²ä¸è¦ãªã‚‚ã®ã€‘
- ã‚«ã‚¿ã‚«ãƒŠèªï¼ˆãƒªãƒªã‚¢ãƒŠã€ã‚¨ãƒªã‚¶ãƒ™ãƒ¼ãƒˆç­‰ï¼‰â†’ TTSã¯æ­£ã—ãèª­ã‚ã‚‹
- ä¸€èˆ¬çš„ãªæ¼¢å­—ï¼ˆç‹å­ã€é­”æ³•ã€è»¢ç”Ÿã€éƒ¨å±‹ã€å½¼å¥³ç­‰ï¼‰â†’ TTSãŒæ­£ã—ãèª­ã‚ã‚‹
- å¥èª­ç‚¹ã®æœ‰ç„¡ã ã‘ã®å·®ç•° â†’ èª­ã¿é–“é•ã„ã§ã¯ãªã„
- WhisperãŒæ–‡ã‚’ã¾ã¨ã‚ãŸå·®ç•° â†’ ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œã®å•é¡Œã§èª­ã¿é–“é•ã„ã§ã¯ãªã„

ã€æ—¢å­˜ã®è¾æ›¸ï¼ˆé‡è¤‡ç™»éŒ²ã—ãªã„ï¼‰ã€‘
{existing_list[:2000]}

ã€å·®ç•°ä¸€è¦§ã€‘
{json.dumps(diff_entries[:80], ensure_ascii=False, indent=2)}

å‡ºåŠ›ã¯JSONå½¢å¼ {{ "æ¼¢å­—èªå¥": "ã²ã‚‰ãŒãªã‚ˆã¿" }} ã®ã¿ã€‚æœ¬å½“ã«TTSãŒé–“é•ãˆãŸç®‡æ‰€ã ã‘ã‚’å³é¸ã—ã¦ãã ã•ã„ã€‚
"""

    print(f"\nğŸ¤– GPT-4oã«{len(meaningful)}ä»¶ã®å·®ç•°ã‚’åˆ†æä¾é ¼ä¸­...")
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={ "type": "json_object" }
    )
    
    new_data = json.loads(response.choices[0].message.content)
    
    if not new_data:
        print("â„¹ï¸ GPTã‹ã‚‰ä¿®æ­£å€™è£œãŒè¿”ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    print(f"ğŸ“‹ GPTã‹ã‚‰{len(new_data)}ä»¶ã®å€™è£œã‚’å—ä¿¡")
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if original_text:
        validated, rejected = validate_corrections(new_data, original_text, existing_corrections)
        
        if rejected:
            print(f"\nâš ï¸ {len(rejected)}ä»¶ã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§é™¤å¤–:")
            for word, reading, reason in rejected:
                print(f"   âŒ {word}: {reading} â†’ {reason}")
    else:
        # åŸæ–‡ãŒãªã„å ´åˆã¯ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç°¡æ˜“ç‰ˆ
        validated = {k: v for k, v in new_data.items() 
                     if isinstance(k, str) and isinstance(v, str) and k.strip() and v.strip()}
        rejected = []
    
    if not validated:
        print("â„¹ï¸ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã€æœ‰åŠ¹ãªä¿®æ­£å€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # è¡¨ç¤º
    print(f"\nâœ… {len(validated)}ä»¶ã®ä¿®æ­£ã‚’é©ç”¨:")
    for word, reading in validated.items():
        print(f"   ğŸ“– {word} â†’ {reading}")
    
    # YAMLæ›´æ–°
    if "corrections" not in data:
        data["corrections"] = {}
    
    data["corrections"].update(validated)
    
    with open(yaml_path, "w", encoding="utf-8") as f:
        yaml.dump(data, f, allow_unicode=True, sort_keys=False, default_flow_style=False)
    
    print(f"\nâœ… {len(validated)}ä»¶ã®ä¿®æ­£ã‚’ {yaml_path} ã«åæ˜ ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="èª­ã¿é–“é•ã„ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰è¾æ›¸ã‚’è‡ªå‹•è£œå¼·")
    parser.add_argument("yaml_path", help="ä¿®æ­£å¯¾è±¡ã®YAMLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--report", help="ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆçœç•¥æ™‚ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¤œç´¢ï¼‰")
    parser.add_argument("--text", help="åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰")
    args = parser.parse_args()
    
    sync_from_report(args.yaml_path, args.report, args.text)
