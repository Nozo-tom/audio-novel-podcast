#!/usr/bin/env python3
"""
å…¨å°èª¬ãƒ†ã‚­ã‚¹ãƒˆè§£æ â†’ ä¸»äººå…¬åãƒ»é »å‡ºèªæŠ½å‡º â†’ TTSèª­ã¿ä¸Šã’ãƒã‚§ãƒƒã‚¯ â†’ ã‚°ãƒ­ãƒ¼ãƒãƒ«è¾æ›¸ç™»éŒ²
"""

import os
import re
import sys
import io
import json
import yaml
import time
import concurrent.futures

# Windowsç’°å¢ƒã§UTF-8å‡ºåŠ›ã‚’å¼·åˆ¶
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
from collections import Counter, defaultdict
from pathlib import Path
from janome.tokenizer import Tokenizer
from dotenv import load_dotenv

# .env ã‹ã‚‰APIã‚­ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# OpenAI
from openai import OpenAI

# â”€â”€â”€ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NOVEL_DIR = Path("novle_input")
CONFIG_PATH = Path("config.yaml")
OUTPUT_REPORT = Path("novel_analysis_report.txt")
BATCH_SIZE = 50  # GPTã«ä¸€åº¦ã«é€ã‚‹èªæ•°
MIN_FREQUENCY = 2  # ã“ã®å›æ•°ä»¥ä¸Šå‡ºç¾ã—ãŸèªã‚’å¯¾è±¡
MAX_WORKERS = 4  # ä¸¦åˆ—å‡¦ç†æ•°

# â”€â”€â”€ Janome ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = Tokenizer()

# â”€â”€â”€ OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = OpenAI()


def load_config():
    """config.yaml ã‚’èª­ã¿è¾¼ã‚€"""
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def load_existing_corrections():
    """æ—¢å­˜ã®è¾æ›¸ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—ï¼ˆconfig.yaml + å€‹åˆ¥YAMLï¼‰"""
    config = load_config()
    corrections = dict(config.get("reading_corrections", {}) or {})
    
    # å€‹åˆ¥YAMLã‹ã‚‰ã‚‚åé›†
    for yaml_file in NOVEL_DIR.glob("*.yaml"):
        try:
            with open(yaml_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)
            if data and "corrections" in data and data["corrections"]:
                corrections.update(data["corrections"])
        except Exception:
            pass
    
    return corrections


def load_all_novels():
    """å…¨å°èª¬ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    novels = {}
    txt_files = sorted(NOVEL_DIR.glob("*.txt"))
    
    if not txt_files:
        print("âŒ novle_input/ ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    print(f"ğŸ“š {len(txt_files)} ä»¶ã®å°èª¬ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    for i, txt_file in enumerate(txt_files, 1):
        try:
            with open(txt_file, "r", encoding="utf-8") as f:
                content = f.read()
            novels[txt_file.name] = content
            # é€²è¡Œåº¦è¡¨ç¤º
            bar = "â–ˆ" * (i * 30 // len(txt_files)) + "â–‘" * (30 - i * 30 // len(txt_files))
            print(f"\r  [{bar}] {i}/{len(txt_files)} èª­è¾¼å®Œäº†", end="", flush=True)
        except Exception as e:
            print(f"\n  âš ï¸ {txt_file.name}: {e}")
    
    print()
    return novels


def extract_character_names_from_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€Œã€å†…ã®è©±è€…ã‚„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã£ã½ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
    names = []
    
    # ã€Œâ—¯â—¯ã¯è¨€ã£ãŸã€ã€Œâ—¯â—¯ãŒå«ã‚“ã ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns = [
        r'([ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ¶ãƒ¼]{2,6})[ã¯ãŒã‚‚ã®](?:è¨€|å«|å‘Ÿ|å›|ç­”|å°‹|è|è©±|ç¬‘|æ³£|æ€’|é©š)',
        r'ã€Œ[^ã€]*ã€\s*(?:ã¨|ã£ã¦)[ã€ã€‚]?\s*([ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ¶ãƒ¼]{2,6})',
        r'([ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ¶ãƒ¼]{2,6})(?:ã•ã‚“|ãã‚“|ã¡ã‚ƒã‚“|æ§˜|æ®¿|å…ˆç”Ÿ|å…ˆè¼©|å¾Œè¼©|ç‹å­|å§«|å…¬çˆµ|ä¼¯çˆµ|ç”·çˆµ|é¨å£«)',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        names.extend(matches)
    
    return names


def analyze_with_janome(text):
    """Janomeã§å½¢æ…‹ç´ è§£æã—ã¦åè©ã‚’æŠ½å‡º"""
    results = {
        "proper_nouns": [],      # å›ºæœ‰åè©
        "person_names": [],      # äººå
        "general_nouns": [],     # ä¸€èˆ¬åè©ï¼ˆæ¼¢å­—å«ã‚€ï¼‰
        "compound_words": [],    # ã‚µå¤‰æ¥ç¶š
    }
    
    tokens = tokenizer.tokenize(text)
    
    for token in tokens:
        surface = token.surface
        part_of_speech = token.part_of_speech.split(",")
        reading = token.reading if token.reading != "*" else None
        
        # 1æ–‡å­—ã¯é™¤å¤–
        if len(surface) <= 1:
            continue
        
        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠã®ã¿ã¯é™¤å¤–
        if re.match(r'^[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼]+$', surface):
            continue
        
        pos_main = part_of_speech[0]
        pos_sub = part_of_speech[1] if len(part_of_speech) > 1 else ""
        
        if pos_main == "åè©":
            if pos_sub == "å›ºæœ‰åè©":
                results["proper_nouns"].append(surface)
            elif pos_sub in ("ä¸€èˆ¬", "ã‚µå¤‰æ¥ç¶š", "å½¢å®¹å‹•è©èªå¹¹"):
                # æ¼¢å­—ã‚’å«ã‚€ã‚‚ã®ã®ã¿
                if re.search(r'[ä¸€-é¾¥]', surface):
                    results["general_nouns"].append(surface)
    
    return results


def analyze_all_novels(novels):
    """å…¨å°èª¬ã‚’è§£æã—ã¦é »å‡ºèªã‚’é›†è¨ˆ"""
    print("\nğŸ” å½¢æ…‹ç´ è§£æä¸­...")
    
    all_proper_nouns = Counter()
    all_general_nouns = Counter()
    all_character_names = Counter()
    novel_appearances = defaultdict(set)  # èª â†’ å‡ºç¾ã—ãŸå°èª¬ã®ã‚»ãƒƒãƒˆ
    
    total = len(novels)
    
    for i, (filename, text) in enumerate(novels.items(), 1):
        bar = "â–ˆ" * (i * 30 // total) + "â–‘" * (30 - i * 30 // total)
        print(f"\r  [{bar}] {i}/{total} è§£æä¸­: {filename[:30]}...", end="", flush=True)
        
        # Janomeè§£æ
        results = analyze_with_janome(text)
        
        for word in results["proper_nouns"]:
            all_proper_nouns[word] += 1
            novel_appearances[word].add(filename)
        
        for word in results["general_nouns"]:
            all_general_nouns[word] += 1
            novel_appearances[word].add(filename)
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        char_names = extract_character_names_from_text(text)
        for name in char_names:
            all_character_names[name] += 1
            novel_appearances[name].add(filename)
    
    print()
    return all_proper_nouns, all_general_nouns, all_character_names, novel_appearances


def check_readability_batch(words, existing_corrections):
    """GPTã§ä¸€æ‹¬ã—ã¦èª­ã¿ã¥ã‚‰ã„èªã‚’ãƒã‚§ãƒƒã‚¯"""
    
    # æ—¢å­˜è¾æ›¸ã«ã‚ã‚‹ã‚‚ã®ã¯é™¤å¤–
    words_to_check = [w for w in words if w not in existing_corrections]
    
    if not words_to_check:
        return {}
    
    prompt = f"""ä»¥ä¸‹ã¯æ—¥æœ¬èªã®å°èª¬ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹å˜èªãƒªã‚¹ãƒˆã§ã™ã€‚
    
OpenAI TTSï¼ˆãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ï¼‰ã§èª­ã¿é–“é•ãˆã‚„ã™ã„å˜èªã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚

åˆ¤å®šåŸºæº–:
1. **äººåãƒ»ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å**: æ¼¢å­—ã®åå‰ã¯å¿…ãšèª­ã¿ãŒå¿…è¦ï¼ˆä¾‹: è’¼çœŸâ†’ãã†ã¾ï¼‰
2. **é›£èª­æ¼¢å­—**: ä¸€èˆ¬çš„ã§ãªã„èª­ã¿ã®èªï¼ˆä¾‹: ä¿¯ã„ãŸâ†’ã†ã¤ã‚€ã„ãŸï¼‰
3. **è¤‡æ•°ã®èª­ã¿æ–¹ãŒã‚ã‚‹èª**: æ–‡è„ˆã§èª­ã¿ãŒå¤‰ã‚ã‚‹ã‚‚ã®ï¼ˆä¾‹: æµè¡Œã‚‹â†’ã¯ã‚„ã‚‹ï¼‰
4. **ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ç”¨èª**: ç•°ä¸–ç•Œãƒ»é­”æ³•ç³»ã®ç‹¬ç‰¹ãªç”¨èª
5. **è¤‡åˆèª**: å€‹åˆ¥ã®èª­ã¿ã§å•é¡Œãªã„èªã¯é™¤å¤–ã—ã¦OK

ä»¥ä¸‹ã®å˜èªã‚’åˆ¤å®šã—ã¦ãã ã•ã„:
{json.dumps(words_to_check, ensure_ascii=False, indent=2)}

èª­ã¿é–“é•ãˆã‚„ã™ã„ã‚‚ã®ã ã‘ã‚’JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
èª­ã¿é–“é•ãˆãªã„ä¸€èˆ¬çš„ãªèªï¼ˆä¾‹: ä¸–ç•Œã€æ™‚é–“ã€é­”æ³•ã€å‹‡è€…ã€èƒ½åŠ› etc.ï¼‰ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
ç‰¹ã«äººåã¯å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ã€ä»–ã®æ–‡å­—ãªã—ï¼‰:
{{"è’¼çœŸ": "ãã†ã¾", "èŠ±éŸ³": "ã‹ã®ã‚“", "ä¿¯ã„ãŸ": "ã†ã¤ã‚€ã„ãŸ"}}

èª­ã¿é–“é•ãˆã‚„ã™ã„ã‚‚ã®ãŒç„¡ã‘ã‚Œã°ç©ºã®JSON {{}} ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯TTSï¼ˆãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚æ—¥æœ¬èªã®TTSãŒèª­ã¿é–“é•ãˆã‚„ã™ã„å˜èªã‚’åˆ¤å®šã—ã¾ã™ã€‚JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
    
    except Exception as e:
        print(f"\n  âš ï¸ GPTã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def check_all_words(all_words, existing_corrections):
    """å…¨å˜èªã‚’ãƒãƒƒãƒã§GPTãƒã‚§ãƒƒã‚¯"""
    print("\nğŸ¤– GPTã§èª­ã¿ä¸Šã’ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    words_list = list(all_words)
    total_batches = (len(words_list) + BATCH_SIZE - 1) // BATCH_SIZE
    all_problematic = {}
    
    for batch_idx in range(total_batches):
        start = batch_idx * BATCH_SIZE
        end = min(start + BATCH_SIZE, len(words_list))
        batch = words_list[start:end]
        
        bar = "â–ˆ" * ((batch_idx + 1) * 30 // total_batches) + "â–‘" * (30 - (batch_idx + 1) * 30 // total_batches)
        print(f"\r  [{bar}] ãƒãƒƒãƒ {batch_idx + 1}/{total_batches} ({start}-{end}/{len(words_list)})", end="", flush=True)
        
        result = check_readability_batch(batch, existing_corrections)
        all_problematic.update(result)
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(0.5)
    
    print()
    return all_problematic


def update_global_dictionary(new_corrections):
    """config.yaml ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«è¾æ›¸ã«è¿½åŠ """
    if not new_corrections:
        print("\nâœ… è¿½åŠ ã™ã¹ãèªã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        content = f.read()
    
    # reading_corrections ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æœ«å°¾ã«è¿½åŠ 
    additions = "\n  # ğŸ” è‡ªå‹•è§£æã§è¿½åŠ ã•ã‚ŒãŸèª­ã¿æ›¿ãˆ\n"
    for word, reading in sorted(new_corrections.items()):
        additions += f'  "{word}": "{reading}"\n'
    
    # reading_corrections ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æœ«å°¾ã‚’è¦‹ã¤ã‘ã¦è¿½åŠ 
    # æœ€å¾Œã®è¡Œã®å¾Œã«è¿½åŠ 
    lines = content.split("\n")
    insert_idx = len(lines)
    
    # reading_corrections ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®æœ€å¾Œã®ã‚¨ãƒ³ãƒˆãƒªã‚’è¦‹ã¤ã‘ã‚‹
    in_corrections = False
    last_correction_idx = -1
    for i, line in enumerate(lines):
        if "reading_corrections:" in line:
            in_corrections = True
            continue
        if in_corrections:
            stripped = line.strip()
            if stripped and not stripped.startswith("#"):
                if ":" in stripped:
                    last_correction_idx = i
            # æ¬¡ã®ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ°é”ã—ãŸã‚‰çµ‚äº†
            if stripped and not stripped.startswith("#") and not stripped.startswith('"') and not stripped.startswith("'") and ":" in stripped and not line.startswith("  "):
                break
    
    if last_correction_idx > 0:
        # æœ€å¾Œã®ã‚¨ãƒ³ãƒˆãƒªã®å¾Œã«æŒ¿å…¥
        lines.insert(last_correction_idx + 1, additions.rstrip())
    else:
        # reading_corrections ã®ç›´å¾Œã«è¿½åŠ 
        for i, line in enumerate(lines):
            if "reading_corrections:" in line:
                lines.insert(i + 1, additions.rstrip())
                break
    
    new_content = "\n".join(lines)
    
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        f.write(new_content)
    
    print(f"\nâœ… config.yaml ã« {len(new_corrections)} ä»¶ã®èª­ã¿æ›¿ãˆã‚’è¿½åŠ ã—ã¾ã—ãŸ")


def generate_report(proper_nouns, general_nouns, character_names, 
                    novel_appearances, problematic_words, existing_corrections):
    """è§£æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    report = []
    report.append("=" * 70)
    report.append("ğŸ“Š å…¨å°èª¬ãƒ†ã‚­ã‚¹ãƒˆè§£æãƒ¬ãƒãƒ¼ãƒˆ")
    report.append("=" * 70)
    report.append("")
    
    # ä¸»äººå…¬ãƒ»ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åå€™è£œ
    report.append("â”€" * 70)
    report.append("ğŸ‘¤ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åå€™è£œï¼ˆå‡ºç¾é »åº¦TOP50ï¼‰")
    report.append("â”€" * 70)
    
    # å›ºæœ‰åè© + ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµ±åˆ
    combined_names = Counter()
    for name, count in proper_nouns.items():
        if re.search(r'[ä¸€-é¾¥]', name) and 2 <= len(name) <= 6:
            combined_names[name] += count
    for name, count in character_names.items():
        combined_names[name] += count
    
    for word, count in combined_names.most_common(50):
        novels = novel_appearances.get(word, set())
        in_dict = "âœ…è¾æ›¸" if word in existing_corrections else ""
        report.append(f"  {word:12s} (å‡ºç¾{count:4d}å›, {len(novels):2d}ä½œå“) {in_dict}")
    
    report.append("")
    
    # é »å‡ºä¸€èˆ¬åè©ï¼ˆæ¼¢å­—å«ã‚€ï¼‰
    report.append("â”€" * 70)
    report.append("ğŸ“ é »å‡ºä¸€èˆ¬åè©TOP50ï¼ˆæ¼¢å­—ã‚’å«ã‚€ï¼‰")
    report.append("â”€" * 70)
    
    for word, count in general_nouns.most_common(50):
        novels = novel_appearances.get(word, set())
        in_dict = "âœ…è¾æ›¸" if word in existing_corrections else ""
        report.append(f"  {word:12s} (å‡ºç¾{count:4d}å›, {len(novels):2d}ä½œå“) {in_dict}")
    
    report.append("")
    
    # TTSèª­ã¿é–“é•ãˆã‚„ã™ã„èª
    report.append("â”€" * 70)
    report.append("âš ï¸ TTSèª­ã¿é–“é•ãˆãƒªã‚¹ã‚¯èªï¼ˆGPTåˆ¤å®šï¼‰")
    report.append("â”€" * 70)
    
    if problematic_words:
        for word, reading in sorted(problematic_words.items()):
            already = "ï¼ˆæ—¢å­˜ï¼‰" if word in existing_corrections else "ğŸ†• æ–°è¦è¿½åŠ "
            report.append(f"  {word} â†’ {reading}  {already}")
    else:
        report.append("  ï¼ˆãªã—ï¼‰")
    
    report.append("")
    report.append("=" * 70)
    
    report_text = "\n".join(report)
    
    with open(OUTPUT_REPORT, "w", encoding="utf-8") as f:
        f.write(report_text)
    
    print(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {OUTPUT_REPORT}")
    return report_text


def main():
    print("=" * 60)
    print("ğŸ” å…¨å°èª¬ãƒ†ã‚­ã‚¹ãƒˆè§£æ â†’ ã‚°ãƒ­ãƒ¼ãƒãƒ«è¾æ›¸æ›´æ–°")
    print("=" * 60)
    
    # 1. æ—¢å­˜è¾æ›¸ã®èª­ã¿è¾¼ã¿
    print("\nğŸ“– æ—¢å­˜è¾æ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    existing_corrections = load_existing_corrections()
    print(f"  æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒª: {len(existing_corrections)} ä»¶")
    
    # 2. å…¨å°èª¬ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿è¾¼ã¿
    novels = load_all_novels()
    total_chars = sum(len(text) for text in novels.values())
    print(f"  åˆè¨ˆæ–‡å­—æ•°: {total_chars:,} æ–‡å­—")
    
    # 3. å½¢æ…‹ç´ è§£æ
    proper_nouns, general_nouns, character_names, novel_appearances = analyze_all_novels(novels)
    
    print(f"\n  ğŸ“Š é›†è¨ˆçµæœ:")
    print(f"    å›ºæœ‰åè©ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {len(proper_nouns)} èª")
    print(f"    ä¸€èˆ¬åè©ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {len(general_nouns)} èª")
    print(f"    ã‚­ãƒ£ãƒ©åå€™è£œ: {len(character_names)} èª")
    
    # 4. ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    # é »åº¦ãŒé«˜ã„ or äººåã£ã½ã„èªã‚’é›†ã‚ã‚‹
    words_to_check = set()
    
    # å›ºæœ‰åè©ã¯å…¨éƒ¨ãƒã‚§ãƒƒã‚¯ï¼ˆ2å›ä»¥ä¸Šå‡ºç¾ï¼‰
    for word, count in proper_nouns.items():
        if count >= MIN_FREQUENCY and re.search(r'[ä¸€-é¾¥]', word):
            words_to_check.add(word)
    
    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åå€™è£œã¯å…¨éƒ¨ãƒã‚§ãƒƒã‚¯
    for word, count in character_names.items():
        if count >= MIN_FREQUENCY and re.search(r'[ä¸€-é¾¥]', word):
            words_to_check.add(word)
    
    # ä¸€èˆ¬åè©ã¯é »å‡ºä¸Šä½200èª
    for word, _ in general_nouns.most_common(200):
        if re.search(r'[ä¸€-é¾¥]', word):
            words_to_check.add(word)
    
    # æ—¢å­˜è¾æ›¸ã«ã‚ã‚‹ã‚‚ã®ã‚’é™¤å¤–
    words_to_check -= set(existing_corrections.keys())
    
    print(f"\n  ğŸ¯ GPTãƒã‚§ãƒƒã‚¯å¯¾è±¡: {len(words_to_check)} èªï¼ˆæ—¢å­˜è¾æ›¸é™¤å¤–æ¸ˆã¿ï¼‰")
    
    # 5. GPTã§èª­ã¿ãƒã‚§ãƒƒã‚¯
    problematic_words = check_all_words(words_to_check, existing_corrections)
    
    print(f"\n  âš ï¸ èª­ã¿é–“é•ãˆãƒªã‚¹ã‚¯èª: {len(problematic_words)} ä»¶")
    
    # 6. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_report(
        proper_nouns, general_nouns, character_names,
        novel_appearances, problematic_words, existing_corrections
    )
    print(report)
    
    # 7. ã‚°ãƒ­ãƒ¼ãƒãƒ«è¾æ›¸æ›´æ–°ç¢ºèª
    new_words = {k: v for k, v in problematic_words.items() if k not in existing_corrections}
    
    if new_words:
        print(f"\nğŸ†• æ–°è¦è¿½åŠ å€™è£œ: {len(new_words)} ä»¶")
        for word, reading in sorted(new_words.items()):
            print(f"  {word} â†’ {reading}")
        
        print(f"\nğŸ’¾ config.yaml ã«è¿½åŠ ã—ã¾ã™ã‹ï¼Ÿ (y/n): ", end="", flush=True)
        answer = input().strip().lower()
        
        if answer == "y":
            update_global_dictionary(new_words)
        else:
            print("â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
    else:
        print("\nâœ… æ–°è¦è¿½åŠ ã™ã¹ãèªã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    print("\nğŸ å®Œäº†ï¼")


if __name__ == "__main__":
    main()
